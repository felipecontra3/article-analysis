{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Jul  1 2016 15:12:24)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "#configure the necessary Spark environment\n",
    "import os, sys\n",
    "\n",
    "#del os.environ['PYSPARK_SUBMIT_ARGS']\n",
    "\n",
    "#here you should set where are located the spark files\n",
    "spark_home = '/dados/app/spark-1.6.2-bin-hadoop2.6/'\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]  = spark_home\n",
    "\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.3-src.zip'))\n",
    "\n",
    "#this defines the sparkContext sc\n",
    "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, math, datetime, shutil\n",
    "from timeit import default_timer as timer\n",
    "import re, unicodedata\n",
    "from nltk.tag import pos_tag \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "\n",
    "from pyspark.sql import Row, SQLContext\n",
    "\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "def createMongoDBConnection(host, port, username, password, db):\n",
    "    client = MongoClient(host, port)\n",
    "    return client[db]\n",
    "\n",
    "def removeAccents(s):\n",
    "  s = ''.join((c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'))\n",
    "  return re.sub(r'[^\\w]', ' ', s)\n",
    "\n",
    "def findProductsByCategory(categories):\n",
    "    \n",
    "    db = createMongoDBConnection(host, port, username, password, database)\n",
    "    produtos = db.produto_novo\n",
    "    \n",
    "    product_list = []\n",
    "    query_filter = {}\n",
    "    if categories:\n",
    "      query_filter = {\"categorias\" : {\"$in\" : categories}}\n",
    "    \n",
    "    for produto in produtos.find(query_filter):\n",
    "        keys = produto.keys()\n",
    "        description = ''\n",
    "        if 'descricaoLonga' in keys:\n",
    "            description = removeAccents(description + produto['descricaoLonga'])\n",
    "        if 'nome' in keys:\n",
    "            description = removeAccents(description + produto ['nome'])\n",
    "        id = None\n",
    "        if '_id' in keys:\n",
    "            id = str(produto['_id'])\n",
    "\n",
    "        category = ''\n",
    "        subcategory = ''\n",
    "\n",
    "        if 'categorias' in keys:\n",
    "            categorias_0 = removeAccents(produto['categorias'][0])\n",
    "\n",
    "            if categorias_0 == \"Inicio\":            \n",
    "                category = removeAccents(produto['categorias'][1])\n",
    "                if(len(produto['categorias']) > 2):\n",
    "                    subcategory = removeAccents(produto['categorias'][2])\n",
    "            else:\n",
    "                category = categorias_0\n",
    "                if(len(produto['categorias']) > 1):\n",
    "                    subcategory = removeAccents(produto['categorias'][1])\n",
    "\n",
    "        if len(description)>0:\n",
    "            product_list.append((id, description, category, subcategory))\n",
    "    \n",
    "    return product_list\n",
    "def insertTokensAndCategories(tokens, category, categoryAndSubcategory):\n",
    "    db = createMongoDBConnection(host, port, username, password, database)\n",
    "\n",
    "    modelCollection = db.model\n",
    "    modelCollection.remove({'_type':'token'})\n",
    "\n",
    "    document_mongo =  dict()\n",
    "    document_mongo['_type'] = 'token'\n",
    "    document_mongo['_datetime'] = datetime.datetime.utcnow()\n",
    "    i = 0\n",
    "    for t in tokens:\n",
    "        document_mongo[t] = i\n",
    "        i = i + 1   \n",
    "\n",
    "    modelCollection.insert_one(document_mongo)\n",
    "\n",
    "    modelCollection.remove({'_type':'category'})\n",
    "\n",
    "    document_mongo =  dict()\n",
    "    document_mongo['_type'] = 'category'\n",
    "    document_mongo['_datetime'] = datetime.datetime.utcnow()\n",
    "    i = 0\n",
    "    for c in category:\n",
    "        document_mongo[c] = i\n",
    "        i = i + 1 \n",
    "\n",
    "    modelCollection.insert_one(document_mongo)\n",
    "\n",
    "    modelCollection.remove({'_type':'category and subcategory'})\n",
    "    \n",
    "    document_mongo =  dict()\n",
    "    document_mongo['_type'] = 'category and subcategory'\n",
    "    document_mongo['_datetime'] = datetime.datetime.utcnow()\n",
    "    i = 0\n",
    "    for c in categoryAndSubcategory:\n",
    "        document_mongo[c[0]+\",\"+c[1]] = i\n",
    "        i = i + 1 \n",
    "\n",
    "    modelCollection.insert_one(document_mongo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "host = 'localhost'\n",
    "port = 27017\n",
    "username = ''\n",
    "password = ''\n",
    "database = 'tcc-recsys-mongo'\n",
    "\n",
    "APP_NAME = 'Recomender System - Treinamento dos Modelos'\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "stpwrds = stopwords.words('english')\n",
    "tbl_translate = dict.fromkeys(i for i in xrange(sys.maxunicode) if unicodedata.category(unichr(i)).startswith('S') or unicodedata.category(unichr(i)).startswith('P') or unicodedata.category(unichr(i)).startswith('N'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Pegando produtos---\n",
      "####levou 12 segundos\n"
     ]
    }
   ],
   "source": [
    "print '---Pegando produtos---'\n",
    "start_i = timer()\n",
    "productRDD = sc.parallelize(findProductsByCategory([]))\n",
    "print '####levou %d segundos' % (timer()-start_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Criando corpus---\n",
      "####levou 0 segundos\n"
     ]
    }
   ],
   "source": [
    "print '---Criando corpus---'\n",
    "start_i = timer()\n",
    "corpusRDD = (productRDD.map(lambda s: (s[0], word_tokenize(s[1].translate(tbl_translate).lower()), s[2], s[3]))\n",
    "                       .map(lambda s: (s[0], [PorterStemmer().stem(x) for x in s[1] if x not in stpwrds], s[2], s[3] ))\n",
    "                       .map(lambda s: (s[0], [x[0] for x in pos_tag(s[1]) if x[1] == 'NN' or x[1] == 'NNP'], s[2], s[3]))\n",
    "                       .cache())\n",
    "print '####levou %d segundos' % (timer()-start_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTokensAndCategories():  \n",
    "    db = createMongoDBConnection(host, port, username, password, database)\n",
    "    model = db.model\n",
    "    \n",
    "    tokens_dict = db.model.find({\"_type\": \"token\"}).limit(1).next()\n",
    "    del tokens_dict['_type']\n",
    "    del tokens_dict['_id']\n",
    "    del tokens_dict['_datetime']\n",
    "    tokens_list = [None] * (max(tokens_dict.values()) + 1)\n",
    "\n",
    "    for key, value in tokens_dict.iteritems():\n",
    "        tokens_list[value] = key\n",
    "\n",
    "    categories_dict = db.model.find({\"_type\": \"category\"}).limit(1).next()\n",
    "    del categories_dict['_type']\n",
    "    del categories_dict['_id']\n",
    "    del categories_dict['_datetime']\n",
    "    categories_list = [None] * (max(categories_dict.values()) + 1)\n",
    "\n",
    "    for key, value in categories_dict.iteritems():\n",
    "        categories_list[value] = key\n",
    "\n",
    "    categories_and_subcategories_dict = db.model.find({\"_type\": \"category and subcategory\"}).limit(1).next()\n",
    "    del categories_and_subcategories_dict['_type']\n",
    "    del categories_and_subcategories_dict['_id']\n",
    "    del categories_and_subcategories_dict['_datetime']\n",
    "    categories_and_subcategories_list = [None] * (max(categories_and_subcategories_dict.values()) + 1)\n",
    "\n",
    "    for key, value in categories_and_subcategories_dict.iteritems():\n",
    "        pre_string = key.split(\",\")\n",
    "        categories_and_subcategories_list[value] = (pre_string[0], pre_string[1])\n",
    "\n",
    "    return tokens_list, categories_list, categories_and_subcategories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens, category, categoryAndSubcategory = getTokensAndCategories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54239\n"
     ]
    }
   ],
   "source": [
    "#print '---Pegando e persistindo dados de categoria e tokens---'\n",
    "# o original que criava essas infos no mongo, porem agora que estou reexecutando, esta dando diferena no numero de tokens\n",
    "#start_i = timer()http://localhost:8888/notebooks/Treinando%20os%20classificadores.ipynb#\n",
    "#tokens = corpusRDD.flatMap(lambda x: x[1]).distinct().collect()\n",
    "numTokens = len(tokens)\n",
    "print numTokens\n",
    "#category = productRDD.map(lambda x: x[2]).distinct().collect()\n",
    "#categoryAndSubcategory = productRDD.map(lambda x: (x[2], x[3])).distinct().collect()\n",
    "#insertTokensAndCategories(tokens, category, categoryAndSubcategory)\n",
    "#print '####levou %d segundos' % (timer()-start_i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Calculando TF-IDF dos produtos---\n",
      "####levou 175 segundos\n"
     ]
    }
   ],
   "source": [
    "print '---Calculando TF-IDF dos produtos---'\n",
    "start_i = timer()\n",
    "wordsData = corpusRDD.map(lambda s: Row(label=s[0], words=s[1], category=s[2], subcategory=s[3]))\n",
    "#persistir isso para que ele nao tenha que fazer de novo na predicaoo\n",
    "wordsDataDF = sqlContext.createDataFrame(wordsData)   \n",
    "\n",
    "wordsDataForPrediction = corpusRDD.map(lambda s: Row(label=s[0], words=s[1], type=s[2], originalpost = ''))\n",
    "#persistir isso para que ele nao tenha que fazer de novo na predicaoo\n",
    "wordsDataForPredictionDF = sqlContext.createDataFrame(wordsDataForPrediction)   \n",
    "\n",
    "if os.path.exists(\"/home/felipe/Documentos/TCC/Experimento/spark_cluster/spark-1.6.2-bin-hadoop2.6/wordsDataDF.parquet\"):\n",
    "    shutil.rmtree(\"/home/felipe/Documentos/TCC/Experimento/spark_cluster/spark-1.6.2-bin-hadoop2.6/wordsDataDF.parquet\")\n",
    "\n",
    "wordsDataForPredictionDF.write.parquet(\"/home/felipe/Documentos/TCC/Experimento/spark_cluster/spark-1.6.2-bin-hadoop2.6/wordsDataDF.parquet\") \n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=numTokens)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "featurizedData = hashingTF.transform(wordsDataDF)\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "#VSM = rescaledData.map(lambda t: LabeledPoint(categoryAndSubcategory.index((t.category, t.subcategory)), t.features))\n",
    "VSM = rescaledData.map(lambda t: LabeledPoint(category.index(t.category), t.features))\n",
    "\n",
    "VSMTrain, VSMTest = VSM.randomSplit([8, 2], seed=0L)\n",
    "print '####levou %d segundos' % (timer()-start_i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Criando modelo Naive Bayes---\n",
      "####levou 71 segundos\n"
     ]
    }
   ],
   "source": [
    "print '--Criando modelo Naive Bayes---'\n",
    "start_i = timer()\n",
    "model = NaiveBayes.train(VSMTrain)\n",
    "\n",
    "if os.path.exists(\"/dados/models/naivebayes/modelo_categoria\"):\n",
    "    shutil.rmtree(\"/dados/models/naivebayes/modelo_categoria\")\n",
    "\n",
    "model.save(sc, '/dados/models/naivebayes/modelo_categoria')\n",
    "print '####levou %d segundos' % (timer()-start_i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Testando modelo Naive Bayes---\n",
      "acuracidade de 93.116783\n",
      "####levou 29 segundos\n"
     ]
    }
   ],
   "source": [
    "print '---Testando modelo Naive Bayes---'\n",
    "start_i = timer()\n",
    "prediction = VSMTest.map(lambda p : (categoryAndSubcategory[int(model.predict(p.features))], categoryAndSubcategory[int(p.label)]))\n",
    "acuraccy = float(prediction.filter(lambda (x, v): x[0]==v[0]).count())/float(prediction.count())\n",
    "print 'acuracidade de %f' % (acuraccy*100)\n",
    "print '####levou %d segundos' % (timer()-start_i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Pegando os posts---\n",
      "####levou 55 segundos\n"
     ]
    }
   ],
   "source": [
    "print '---Pegando os posts---'\n",
    "start_i = timer()\n",
    "posts = list()\n",
    "wb = load_workbook(filename = '/home/felipe/Documentos/TCC/Experimento/ml_module/base_sentimentos.xlsx')\n",
    "sheet = wb['Menes']\n",
    "for row in sheet.iter_rows(row_offset=1):\n",
    "    post = list()\n",
    "    for cell in row:\n",
    "        if cell.value is None:\n",
    "            break\n",
    "        post.append(1 if cell.value == 'Positive' or cell.value == 'Neutral' else 0 if cell.value == 'Negative' else removeAccents(cell.value))\n",
    "\n",
    "    if len(post) > 0:            \n",
    "        posts.append(tuple(post))\n",
    "\n",
    "print '####levou %d segundos' % (timer()-start_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Criando corpus---\n",
      "####levou 0 segundos\n"
     ]
    }
   ],
   "source": [
    "print '---Criando corpus---'\n",
    "start_i = timer()\n",
    "postsRDD = sc.parallelize(posts)\n",
    "postCorpusRDD = (postsRDD.map(lambda s: (s[1], word_tokenize(s[0].translate(tbl_translate).lower())))\n",
    "                       .map(lambda s: (s[0], [PorterStemmer().stem(x) for x in s[1] if x not in stpwrds]))\n",
    "                       .map(lambda s: (s[0], [x[0] for x in pos_tag(s[1]) if x[1] == 'NN' or x[1] == 'NNP']))\n",
    "                       .cache())\n",
    "\n",
    "print '####levou %d segundos' % (timer()-start_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Calculando TF-IDF dos Posts---\n",
      "####levou 41 segundos\n"
     ]
    }
   ],
   "source": [
    "print '---Calculando TF-IDF dos Posts---'\n",
    "start_i = timer()\n",
    "wordsData = postCorpusRDD.map(lambda s: Row(label=s[0], words=s[1]))\n",
    "wordsDataDF = sqlContext.createDataFrame(wordsData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=numTokens)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "featurizedData = hashingTF.transform(wordsDataDF)\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)   \n",
    "\n",
    "VSM = rescaledData.map(lambda t: LabeledPoint(t.label, t.features))\n",
    "VSMTrain, VSMTest = VSM.randomSplit([8, 2], seed=0L)\n",
    "print '####levou %d segundos' % (timer()-start_i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Criando modelo SVM---\n",
      "####levou 17 segundos\n"
     ]
    }
   ],
   "source": [
    "print '--Criando modelo SVM---'\n",
    "start_i = timer()\n",
    "model = SVMWithSGD.train(VSMTrain, iterations=100)\n",
    "\n",
    "if os.path.exists(\"/dados/models/svm\"):\n",
    "    shutil.rmtree(\"/dados/models/svm\")\n",
    "\n",
    "model.save(sc, \"/dados/models/svm\")\n",
    "print '####levou %d segundos' % (timer()-start_i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Testando modelo SVM---\n",
      "acuracidade de 80.262046\n",
      "####levou 5 segundos\n"
     ]
    }
   ],
   "source": [
    "print '---Testando modelo SVM---'\n",
    "start_i = timer()\n",
    "prediction = VSMTest.map(lambda p: (int(p.label), model.predict(p.features)))\n",
    "acuraccy = prediction.filter(lambda (v, p): v == p).count() / float(prediction.count())\n",
    "\n",
    "print 'acuracidade de %f' % (acuraccy*100)\n",
    "print '####levou %d segundos' % (timer()-start_i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
